---
header-includes: \usepackage{color}
output:
  html_document: default
  pdf_document:
    fig_caption: no
---
<style>
table, th, td { text-align:right; }
th, td {padding: 10px;}
</style>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
whichcomp <- strsplit(getwd(),"/")[[1]][3]
load(paste0("c:/users/", whichcomp, "/Dropbox/Resma3/Resma3.RData"))
library(knitr)
opts_chunk$set(fig.width=6, fig.align = "center", 
      out.width = "70%", warning=FALSE, message=FALSE)
library(ggplot2)
library(grid)
```
`r rmd$basefontsize()`

#Normal (Gaussian) Distribution

Named after [Karl Friedrich Gauss](href="http://en.wikipedia.org/wiki/Carl_Friedrich_Gauss")

![](graphs/gauss.jpg)

This (for good reasons we will see shortly) is the most important distribution of them all! First, it is already familiar to you because it results in data with bell-shaped histograms:

```{r echo=FALSE}
hplot(rnorm(1000))
```

A normal random distribution has two parameters, denoted by $\mu$ and $\sigma$.  

What is the meaning (interpretation) of the parameters? It is of course that $\mu$  is the **population mean **and $\sigma$ is the **population standard deviation**.

**Example**: In the next picture we have 4 examples of normal densities with different means and standard deviations, drawn on the same scale: 

```{r, warning=FALSE, echo=FALSE}
x <- seq(0, 10, length=1000)
plt1 <- splot(dnorm(x, 5, 2), x, plotPoints = FALSE,  label_x = "x", label_y = "", addLine = 3, return.graph = TRUE)
plt1 <- plt1 + geom_line() +
  ggtitle(expression(paste(mu, "=5, ", sigma, "=3")))
plt2 <- splot(dnorm(x, 5, 0.5), x, label_x = "x", plotPoints = FALSE,label_y = "", addLine = 3, return.graph = TRUE)
plt2 <- plt2 + geom_line() +
  ggtitle(expression(paste(mu, "=5, ", sigma, "=0.5")))
plt3 <- splot(dnorm(x, 3, 2), x, label_x = "x", label_y = "", addLine = 3, plotPoints = FALSE, return.graph = TRUE)
plt3 <- plt3 + geom_line() +
  ggtitle(expression(paste(mu, "=3, ", sigma, "=2")))
plt4 <- splot(dnorm(x, 8, 0.2), plotPoints = FALSE,x, label_x = "x", label_y = "", addLine = 3, return.graph = TRUE)
plt4 <- plt4 + geom_line() +
  ggtitle(expression(paste(mu, "=8, ", sigma, "=0.2")))
multiple.graphs(plt1, plt2, plt3, plt4)
```


###App

```{r eval=FALSE}
run.app(normal)
```

this app draws the histogram of data from a normal distribution with different means and standard deviations.

##Central Limit Theorem

Why is the normal distribution so important? The reason is the **Central Limit Theorem**, which states that under some very general conditions the **sample mean** has (approximately) a normal distribution, no matter what the distribution of the observations.

**Example** As an illustration let's do the following. We start by getting some data that is very much NOT normally distributed. I have a routine to that called **clt_illustration**. To see what the data looks like run

```{r}
clt.illustration(1)
```

Now this clearly is not a bell-shaped histogram! 
Now let's do the following: generate pairs of numbers x1, x2 and find their mean with (x1+x2)/2. We can do this with 
  
```{r}
clt.illustration(2) 
```

Still not much of a bell-shaped histogram. But if we keep numbers to the mean we quickly get there, here is what it looks like for 10:

```{r}
clt.illustration(3) 
```

```{r}
clt.illustration(10) 
```

There is very nice way to illustrate the workings of the central limit theorem called a [Galton Board Video](http://www.youtube.com/watch?v=3m4bxse2JEQ)

###App

this app does various illustrations of the central limit theorem 

```{r eval=FALSE}
run.app(clt)
```

##Theory of Errors

In real life almost any measuring device makes some errors. Some instruments are lousy and make big ones, other instruments are excellent and make small ones.

**Example** You want to measure the length a certain streetlight is red. You ask 10 friends to go with you and everyone makes a guess.

**Example** You want to measure the length a certain streetlight is red. You ask 10 friends to go with you. You have a stopwatch that you give to each friend.

Clearly in the second case we expect to get much smaller errors.

Around 1800 Gauss was thinking about what one could say in great generality about such measurment errors. He came up with the following rules that (almost) all measurement errors should follow, no matter what the instrument:

1.  Small errors are more likely than large errors.

2.  an error of $\epsilon$ is just as likely as an error of $-\epsilon$

3.  In the presence of several measurements of the same quantity, the most likely value of the quantity being measured is their average.

Now it is quite astonishing that JUST FROM THESE THREE rules he was able to derive the normal curve.

For the math people, the mathematical function is

$$
\frac1{\sqrt{2\pi \sigma^2}} e^{ -(x-\mu)^2/(2\sigma^2) }
$$

notice it has two very famous math numbers in it, $\pi$ and $e$
For more on the central limit theorem see page 407 of the textbook.
